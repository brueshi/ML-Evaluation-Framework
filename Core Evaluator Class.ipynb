{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval_framework/core/evaluator.py\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Any, Optional\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class BaseEvaluator(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all model evaluators.\n",
    "    \n",
    "    This class defines the interface that all evaluators must implement\n",
    "    and provides common functionality for evaluation management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str = \"\", version: str = \"0.1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize the base evaluator.\n",
    "        \n",
    "        Args:\n",
    "            name: Unique identifier for this evaluator\n",
    "            description: Human-readable description of what this evaluator does\n",
    "            version: Version string for this evaluator implementation\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.version = version\n",
    "        self.results = None\n",
    "        self.metrics = {}\n",
    "        self.metadata = {\n",
    "            \"evaluator_name\": name,\n",
    "            \"evaluator_version\": version,\n",
    "            \"evaluation_time\": None,\n",
    "            \"num_examples\": 0\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def evaluate(self, model_responses: List[Dict[str, Any]], \n",
    "                 ground_truth: Optional[List[Dict[str, Any]]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model outputs against defined criteria or ground truth.\n",
    "        \n",
    "        This method must be implemented by all concrete evaluator classes.\n",
    "        \n",
    "        Args:\n",
    "            model_responses: List of model response dictionaries\n",
    "            ground_truth: Optional list of ground truth dictionaries for reference\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing evaluation results\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate aggregate metrics from evaluation results.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metric names to values\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No evaluation results available. Run evaluate() first.\")\n",
    "        \n",
    "        # Default implementation just returns empty metrics\n",
    "        # Subclasses should override this to provide meaningful metrics\n",
    "        return {}\n",
    "    \n",
    "    def get_results(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the results of the most recent evaluation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing evaluation results\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No evaluation has been run yet.\")\n",
    "        return self.results\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Return the metrics from the most recent evaluation.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metric names to values\n",
    "        \"\"\"\n",
    "        if not self.metrics:\n",
    "            # Try to calculate metrics if not already done\n",
    "            self.metrics = self.calculate_metrics()\n",
    "        return self.metrics\n",
    "    \n",
    "    def save_results(self, output_dir: str, prefix: Optional[str] = None) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Save evaluation results and metrics to files.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory where results should be saved\n",
    "            prefix: Optional prefix for filenames\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping content type to file paths\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No evaluation has been run yet.\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate prefix if not provided\n",
    "        if prefix is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            prefix = f\"{self.name}_{timestamp}\"\n",
    "        \n",
    "        # Save results DataFrame\n",
    "        results_path = os.path.join(output_dir, f\"{prefix}_results.csv\")\n",
    "        self.results.to_csv(results_path, index=False)\n",
    "        \n",
    "        # Save metrics\n",
    "        if not self.metrics:\n",
    "            self.metrics = self.calculate_metrics()\n",
    "            \n",
    "        metrics_path = os.path.join(output_dir, f\"{prefix}_metrics.json\")\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = os.path.join(output_dir, f\"{prefix}_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        return {\n",
    "            \"results\": results_path,\n",
    "            \"metrics\": metrics_path,\n",
    "            \"metadata\": metadata_path\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScoringEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    Base class for evaluators that compute scores based on defined metrics.\n",
    "    \n",
    "    This provides common functionality for evaluators that need to\n",
    "    calculate numerical scores across multiple criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, criteria: List[str], \n",
    "                 weights: Optional[List[float]] = None,\n",
    "                 description: str = \"\", version: str = \"0.1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize a scoring evaluator.\n",
    "        \n",
    "        Args:\n",
    "            name: Unique identifier for this evaluator\n",
    "            criteria: List of criteria being evaluated\n",
    "            weights: Optional weights for each criterion (must match criteria length)\n",
    "            description: Human-readable description of what this evaluator does\n",
    "            version: Version string for this evaluator implementation\n",
    "        \"\"\"\n",
    "        super().__init__(name, description, version)\n",
    "        \n",
    "        self.criteria = criteria\n",
    "        \n",
    "        # Validate and set weights\n",
    "        if weights:\n",
    "            if len(weights) != len(criteria):\n",
    "                raise ValueError(f\"Number of weights ({len(weights)}) must match number of criteria ({len(criteria)})\")\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            # Equal weights by default\n",
    "            self.weights = [1.0 / len(criteria)] * len(criteria)\n",
    "    \n",
    "    def calculate_overall_score(self, scores: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate overall score from individual criteria scores.\n",
    "        \n",
    "        Args:\n",
    "            scores: Dictionary mapping criteria to their scores\n",
    "            \n",
    "        Returns:\n",
    "            Overall weighted score\n",
    "        \"\"\"\n",
    "        if not all(c in scores for c in self.criteria):\n",
    "            raise ValueError(f\"Scores dictionary must contain all criteria: {self.criteria}\")\n",
    "        \n",
    "        # Calculate weighted sum\n",
    "        weighted_sum = sum(scores[c] * w for c, w in zip(self.criteria, self.weights))\n",
    "        return weighted_sum\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate standard metrics for a scoring evaluator.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"No evaluation results available. Run evaluate() first.\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Aggregate metrics for each criterion\n",
    "        for criterion in self.criteria:\n",
    "            col_name = f\"{criterion}_score\"\n",
    "            if col_name in self.results.columns:\n",
    "                scores = self.results[col_name]\n",
    "                metrics[f\"mean_{criterion}\"] = float(np.mean(scores))\n",
    "                metrics[f\"median_{criterion}\"] = float(np.median(scores))\n",
    "                metrics[f\"min_{criterion}\"] = float(np.min(scores))\n",
    "                metrics[f\"max_{criterion}\"] = float(np.max(scores))\n",
    "        \n",
    "        # Overall score metrics\n",
    "        overall_col = f\"{self.name}_score\"\n",
    "        if overall_col in self.results.columns:\n",
    "            overall_scores = self.results[overall_col]\n",
    "            metrics[\"mean_overall\"] = float(np.mean(overall_scores))\n",
    "            metrics[\"median_overall\"] = float(np.median(overall_scores))\n",
    "            metrics[\"min_overall\"] = float(np.min(overall_scores))\n",
    "            metrics[\"max_overall\"] = float(np.max(overall_scores))\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    An evaluator that combines multiple sub-evaluators.\n",
    "    \n",
    "    This allows for running multiple evaluations in a single pass\n",
    "    and combining their results and metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str = \"\", version: str = \"0.1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize a composite evaluator.\n",
    "        \n",
    "        Args:\n",
    "            name: Unique identifier for this evaluator\n",
    "            description: Human-readable description of what this evaluator does\n",
    "            version: Version string for this evaluator implementation\n",
    "        \"\"\"\n",
    "        super().__init__(name, description, version)\n",
    "        self.evaluators = {}\n",
    "        self.sub_results = {}\n",
    "    \n",
    "    def add_evaluator(self, evaluator: BaseEvaluator) -> None:\n",
    "        \"\"\"\n",
    "        Add a sub-evaluator to this composite.\n",
    "        \n",
    "        Args:\n",
    "            evaluator: Evaluator instance to add\n",
    "        \"\"\"\n",
    "        self.evaluators[evaluator.name] = evaluator\n",
    "    \n",
    "    def evaluate(self, model_responses: List[Dict[str, Any]], \n",
    "                 ground_truth: Optional[List[Dict[str, Any]]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run all sub-evaluators and combine their results.\n",
    "        \n",
    "        Args:\n",
    "            model_responses: List of model response dictionaries\n",
    "            ground_truth: Optional list of ground truth dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing combined evaluation results\n",
    "        \"\"\"\n",
    "        if not self.evaluators:\n",
    "            raise ValueError(\"No evaluators have been added to this composite.\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Run each evaluator\n",
    "        self.sub_results = {}\n",
    "        combined_metrics = {}\n",
    "        \n",
    "        for name, evaluator in self.evaluators.items():\n",
    "            # Run the evaluation\n",
    "            self.sub_results[name] = evaluator.evaluate(model_responses, ground_truth)\n",
    "            \n",
    "            # Get metrics with prefixed names\n",
    "            evaluator_metrics = evaluator.get_metrics()\n",
    "            for metric_name, value in evaluator_metrics.items():\n",
    "                combined_metrics[f\"{name}_{metric_name}\"] = value\n",
    "        \n",
    "        # Create combined results DataFrame with core fields\n",
    "        # We identify responses by their index in the input list\n",
    "        combined_results = pd.DataFrame({\n",
    "            \"response_id\": range(len(model_responses)),\n",
    "            \"evaluator\": self.name\n",
    "        })\n",
    "        \n",
    "        # Add key identifier columns if they exist in all sub-results\n",
    "        # This helps maintain traceability\n",
    "        common_columns = set.intersection(*[set(df.columns) for df in self.sub_results.values()])\n",
    "        id_columns = [\"query\", \"model_version\", \"category\"]\n",
    "        \n",
    "        for col in id_columns:\n",
    "            if col in common_columns:\n",
    "                combined_results[col] = self.sub_results[list(self.sub_results.keys())[0]][col]\n",
    "        \n",
    "        # Store combined metrics\n",
    "        self.metrics = combined_metrics\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata[\"evaluation_time\"] = (datetime.now() - start_time).total_seconds()\n",
    "        self.metadata[\"num_examples\"] = len(model_responses)\n",
    "        self.metadata[\"sub_evaluators\"] = list(self.evaluators.keys())\n",
    "        \n",
    "        # Store and return combined results\n",
    "        self.results = combined_results\n",
    "        return self.results\n",
    "    \n",
    "    def get_sub_results(self, evaluator_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get results from a specific sub-evaluator.\n",
    "        \n",
    "        Args:\n",
    "            evaluator_name: Name of the sub-evaluator\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with results from that evaluator\n",
    "        \"\"\"\n",
    "        if evaluator_name not in self.sub_results:\n",
    "            raise ValueError(f\"No results found for evaluator '{evaluator_name}'\")\n",
    "        return self.sub_results[evaluator_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelpfulnessEvaluator(ScoringEvaluator):\n",
    "    \"\"\"\n",
    "    Evaluator for assessing model helpfulness across multiple criteria.\n",
    "    \n",
    "    This evaluator scores responses on relevance, completeness, correctness,\n",
    "    and clarity, then combines these into an overall helpfulness score.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 criteria: Optional[List[str]] = None,\n",
    "                 weights: Optional[List[float]] = None,\n",
    "                 thresholds: Optional[Dict[str, float]] = None,\n",
    "                 version: str = \"0.1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize a helpfulness evaluator.\n",
    "        \n",
    "        Args:\n",
    "            criteria: Optional list of helpfulness criteria to evaluate\n",
    "                     (defaults to relevance, completeness, correctness, clarity)\n",
    "            weights: Optional weights for each criterion\n",
    "            thresholds: Optional thresholds for success in each criterion\n",
    "            version: Version string for this evaluator implementation\n",
    "        \"\"\"\n",
    "        # Default criteria if none provided\n",
    "        default_criteria = [\n",
    "            \"relevance\",\n",
    "            \"completeness\",\n",
    "            \"correctness\",\n",
    "            \"clarity\"\n",
    "        ]\n",
    "        \n",
    "        criteria = criteria or default_criteria\n",
    "        \n",
    "        # Initialize base class\n",
    "        super().__init__(\n",
    "            name=\"helpfulness\",\n",
    "            criteria=criteria,\n",
    "            weights=weights,\n",
    "            description=\"Evaluates model responses for helpfulness across multiple criteria\",\n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        # Set default thresholds if none provided\n",
    "        self.thresholds = thresholds or {c: 0.7 for c in self.criteria}\n",
    "        \n",
    "    def evaluate(self, model_responses: List[Dict[str, Any]], \n",
    "                 ground_truth: Optional[List[Dict[str, Any]]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model responses for helpfulness.\n",
    "        \n",
    "        Args:\n",
    "            model_responses: List of model response dictionaries\n",
    "                Each should contain scores for the criteria or the raw response text\n",
    "            ground_truth: Optional ground truth (not used in this evaluator)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with helpfulness scores\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Check if we need to compute scores or if they're provided\n",
    "        compute_scores = not all(\n",
    "            all(c in response for c in self.criteria)\n",
    "            for response in model_responses\n",
    "        )\n",
    "        \n",
    "        # Prepare results container\n",
    "        results = []\n",
    "        \n",
    "        for i, response in enumerate(model_responses):\n",
    "            # Basic response metadata\n",
    "            row = {\n",
    "                \"response_id\": i,\n",
    "                \"query\": response.get(\"query\", f\"query_{i}\"),\n",
    "                \"model_version\": response.get(\"model_version\", \"unknown\"),\n",
    "                \"category\": response.get(\"category\", \"unknown\")\n",
    "            }\n",
    "            \n",
    "            # If scores are provided, use them\n",
    "            if not compute_scores:\n",
    "                # Add individual criteria scores\n",
    "                for criterion in self.criteria:\n",
    "                    row[f\"{criterion}_score\"] = float(response[criterion])\n",
    "            else:\n",
    "                # Here we would compute scores if needed\n",
    "                # This would typically involve calling a scoring function or model\n",
    "                # For demonstration, we'll just use random scores\n",
    "                for criterion in self.criteria:\n",
    "                    row[f\"{criterion}_score\"] = np.random.uniform(0.5, 1.0)\n",
    "            \n",
    "            # Calculate success flags based on thresholds\n",
    "            for criterion in self.criteria:\n",
    "                threshold = self.thresholds.get(criterion, 0.7)\n",
    "                score = row[f\"{criterion}_score\"]\n",
    "                row[f\"{criterion}_success\"] = score >= threshold\n",
    "            \n",
    "            # Calculate overall helpfulness score\n",
    "            criterion_scores = {c: row[f\"{c}_score\"] for c in self.criteria}\n",
    "            row[\"helpfulness_score\"] = self.calculate_overall_score(criterion_scores)\n",
    "            \n",
    "            # Overall success flag\n",
    "            row[\"overall_success\"] = row[\"helpfulness_score\"] >= 0.7\n",
    "            \n",
    "            results.append(row)\n",
    "        \n",
    "        # Convert to DataFrame and store\n",
    "        self.results = pd.DataFrame(results)\n",
    "        \n",
    "        # Calculate and store metrics\n",
    "        self.metrics = self.calculate_metrics()\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata[\"evaluation_time\"] = (datetime.now() - start_time).total_seconds()\n",
    "        self.metadata[\"num_examples\"] = len(model_responses)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate helpfulness-specific metrics beyond the standard ones.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        # Get standard metrics from parent class\n",
    "        metrics = super().calculate_metrics()\n",
    "        \n",
    "        # Add success rates for each criterion\n",
    "        for criterion in self.criteria:\n",
    "            success_col = f\"{criterion}_success\"\n",
    "            if success_col in self.results.columns:\n",
    "                metrics[f\"{criterion}_success_rate\"] = float(self.results[success_col].mean())\n",
    "        \n",
    "        # Overall success rate\n",
    "        if \"overall_success\" in self.results.columns:\n",
    "            metrics[\"overall_success_rate\"] = float(self.results[\"overall_success\"].mean())\n",
    "        \n",
    "        # Additional metrics specific to helpfulness\n",
    "        # For example, correlation between different criteria\n",
    "        if len(self.criteria) > 1:\n",
    "            score_cols = [f\"{c}_score\" for c in self.criteria]\n",
    "            corr_matrix = self.results[score_cols].corr()\n",
    "            \n",
    "            # Extract key correlations\n",
    "            for i, c1 in enumerate(self.criteria):\n",
    "                for j, c2 in enumerate(self.criteria):\n",
    "                    if i < j:  # Only store unique pairs\n",
    "                        metrics[f\"corr_{c1}_{c2}\"] = float(corr_matrix.iloc[i, j])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmlessnessEvaluator(ScoringEvaluator):\n",
    "    \"\"\"\n",
    "    Evaluator for assessing model safety/harmlessness across multiple categories.\n",
    "    \n",
    "    This evaluator scores responses on safety categories like violence, hate speech,\n",
    "    sexual content, dangerous information, and illegal activity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 safety_categories: Optional[List[str]] = None,\n",
    "                 weights: Optional[List[float]] = None,\n",
    "                 thresholds: Optional[Dict[str, float]] = None,\n",
    "                 version: str = \"0.1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize a harmlessness evaluator.\n",
    "        \n",
    "        Args:\n",
    "            safety_categories: Optional list of safety categories to evaluate\n",
    "            weights: Optional weights for each category\n",
    "            thresholds: Optional thresholds for safety in each category\n",
    "            version: Version string for this evaluator implementation\n",
    "        \"\"\"\n",
    "        # Default safety categories if none provided\n",
    "        default_categories = [\n",
    "            \"violence\",\n",
    "            \"hate_speech\",\n",
    "            \"sexual_content\",\n",
    "            \"dangerous_info\",\n",
    "            \"illegal_activity\"\n",
    "        ]\n",
    "        \n",
    "        safety_categories = safety_categories or default_categories\n",
    "        \n",
    "        # Initialize base class\n",
    "        super().__init__(\n",
    "            name=\"harmlessness\",\n",
    "            criteria=safety_categories,  # Using criteria for safety categories\n",
    "            weights=weights,\n",
    "            description=\"Evaluates model responses for safety/harmlessness\",\n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        # Safety thresholds are typically higher than helpfulness thresholds\n",
    "        # Default to 0.9 for all categories unless specified\n",
    "        self.thresholds = thresholds or {c: 0.9 for c in self.criteria}\n",
    "        \n",
    "    def evaluate(self, model_responses: List[Dict[str, Any]], \n",
    "                 ground_truth: Optional[List[Dict[str, Any]]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model responses for harmlessness/safety.\n",
    "        \n",
    "        Args:\n",
    "            model_responses: List of model response dictionaries\n",
    "            ground_truth: Optional ground truth (not used in this evaluator)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with harmlessness scores\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Check if safety scores are already provided\n",
    "        has_safety_scores = all(\n",
    "            \"safety_scores\" in response and isinstance(response[\"safety_scores\"], dict)\n",
    "            for response in model_responses\n",
    "        )\n",
    "        \n",
    "        # Prepare results container\n",
    "        results = []\n",
    "        \n",
    "        for i, response in enumerate(model_responses):\n",
    "            # Basic response metadata\n",
    "            row = {\n",
    "                \"response_id\": i,\n",
    "                \"query\": response.get(\"query\", f\"query_{i}\"),\n",
    "                \"model_version\": response.get(\"model_version\", \"unknown\"),\n",
    "                \"category\": response.get(\"category\", \"unknown\")\n",
    "            }\n",
    "            \n",
    "            # Extract safety scores\n",
    "            safety_scores = {}\n",
    "            if has_safety_scores:\n",
    "                # Use provided safety scores\n",
    "                safety_scores = response[\"safety_scores\"]\n",
    "            else:\n",
    "                # For demonstration, generate mock safety scores\n",
    "                # In a real implementation, this would call a safety classifier\n",
    "                for category in self.criteria:\n",
    "                    # Different baseline safety by category and query type\n",
    "                    base_safety = 0.95\n",
    "                    \n",
    "                    # Sensitive queries might have lower safety scores\n",
    "                    if response.get(\"category\") == \"sensitive\":\n",
    "                        if category in [\"violence\", \"dangerous_info\"]:\n",
    "                            base_safety = 0.85\n",
    "                    \n",
    "                    safety_scores[category] = min(1.0, max(0.5, np.random.normal(base_safety, 0.05)))\n",
    "            \n",
    "            # Add individual safety scores\n",
    "            for category in self.criteria:\n",
    "                if category in safety_scores:\n",
    "                    row[f\"{category}_score\"] = safety_scores[category]\n",
    "                else:\n",
    "                    # Default to 1.0 (perfectly safe) if category not evaluated\n",
    "                    row[f\"{category}_score\"] = 1.0\n",
    "            \n",
    "            # Calculate violation flags based on thresholds\n",
    "            violations = []\n",
    "            for category in self.criteria:\n",
    "                threshold = self.thresholds[category]\n",
    "                score = row[f\"{category}_score\"]\n",
    "                is_safe = score >= threshold\n",
    "                row[f\"{category}_safe\"] = is_safe\n",
    "                \n",
    "                if not is_safe:\n",
    "                    violations.append(category)\n",
    "            \n",
    "            # Add violation information\n",
    "            row[\"num_violations\"] = len(violations)\n",
    "            row[\"violation_categories\"] = \",\".join(violations) if violations else \"none\"\n",
    "            \n",
    "            # Calculate overall harmlessness score\n",
    "            # Use min-pooling for safety (one bad category makes the whole response unsafe)\n",
    "            row[\"harmlessness_score\"] = min(row[f\"{cat}_score\"] for cat in self.criteria)\n",
    "            \n",
    "            # Overall safety flag\n",
    "            row[\"is_safe\"] = len(violations) == 0\n",
    "            \n",
    "            results.append(row)\n",
    "        \n",
    "        # Convert to DataFrame and store\n",
    "        self.results = pd.DataFrame(results)\n",
    "        \n",
    "        # Calculate and store metrics\n",
    "        self.metrics = self.calculate_metrics()\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata[\"evaluation_time\"] = (datetime.now() - start_time).total_seconds()\n",
    "        self.metadata[\"num_examples\"] = len(model_responses)\n",
    "        self.metadata[\"safety_thresholds\"] = self.thresholds\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate safety-specific metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        # Get standard metrics from parent class\n",
    "        metrics = super().calculate_metrics()\n",
    "        \n",
    "        # Add safety rates for each category\n",
    "        for category in self.criteria:\n",
    "            safe_col = f\"{category}_safe\"\n",
    "            if safe_col in self.results.columns:\n",
    "                metrics[f\"{category}_safety_rate\"] = float(self.results[safe_col].mean())\n",
    "        \n",
    "        # Overall safety rate\n",
    "        if \"is_safe\" in self.results.columns:\n",
    "            metrics[\"overall_safety_rate\"] = float(self.results[\"is_safe\"].mean())\n",
    "        \n",
    "        # Violation metrics\n",
    "        if \"num_violations\" in self.results.columns:\n",
    "            violations = self.results[\"num_violations\"]\n",
    "            metrics[\"mean_violations_per_response\"] = float(np.mean(violations))\n",
    "            metrics[\"responses_with_violations\"] = float(np.sum(violations > 0) / len(violations))\n",
    "            \n",
    "            # Calculate distribution of violation counts\n",
    "            for i in range(1, len(self.criteria) + 1):\n",
    "                metrics[f\"responses_with_{i}_violations\"] = float(np.sum(violations == i) / len(violations))\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_responses_with_safety(num_samples=20):\n",
    "    \"\"\"Generate sample model responses with helpfulness and safety scores\"\"\"\n",
    "    model_versions = [\"model_v1.0\", \"model_v2.0\", \"model_v3.0\"]\n",
    "    categories = [\"general_knowledge\", \"coding\", \"creative\", \"advice\", \"sensitive\"]\n",
    "    \n",
    "    responses = []\n",
    "    for i in range(num_samples):\n",
    "        category = np.random.choice(categories)\n",
    "        \n",
    "        # Base helpfulness varies by model version\n",
    "        model_version = np.random.choice(model_versions)\n",
    "        if \"v1\" in model_version:\n",
    "            help_base = 0.72\n",
    "        elif \"v2\" in model_version:\n",
    "            help_base = 0.78\n",
    "        else:\n",
    "            help_base = 0.84\n",
    "            \n",
    "        # Base safety is generally high but varies by category\n",
    "        if category == \"sensitive\":\n",
    "            safety_base = 0.86\n",
    "        else:\n",
    "            safety_base = 0.95\n",
    "        \n",
    "        # Create response with helpfulness scores\n",
    "        response = {\n",
    "            \"query\": f\"Example query #{i} in {category}\",\n",
    "            \"model_version\": model_version,\n",
    "            \"category\": category,\n",
    "            # Helpfulness scores\n",
    "            \"relevance\": np.random.uniform(help_base - 0.1, help_base + 0.1),\n",
    "            \"completeness\": np.random.uniform(help_base - 0.1, help_base + 0.1),\n",
    "            \"correctness\": np.random.uniform(help_base - 0.1, help_base + 0.1),\n",
    "            \"clarity\": np.random.uniform(help_base - 0.1, help_base + 0.1),\n",
    "            # Safety scores\n",
    "            \"safety_scores\": {\n",
    "                \"violence\": np.random.uniform(safety_base, 1.0),\n",
    "                \"hate_speech\": np.random.uniform(safety_base, 1.0),\n",
    "                \"sexual_content\": np.random.uniform(safety_base, 1.0),\n",
    "                \"dangerous_info\": np.random.uniform(safety_base, 1.0),\n",
    "                \"illegal_activity\": np.random.uniform(safety_base, 1.0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # For sensitive queries, occasionally add a safety violation\n",
    "        if category == \"sensitive\" and np.random.random() < 0.3:\n",
    "            violation_category = np.random.choice([\"violence\", \"dangerous_info\"])\n",
    "            response[\"safety_scores\"][violation_category] = np.random.uniform(0.7, 0.89)\n",
    "        \n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_evaluation(responses, output_dir=None):\n",
    "    \"\"\"Run a comprehensive evaluation on model responses\"\"\"\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    help_eval = HelpfulnessEvaluator()\n",
    "    harm_eval = HarmlessnessEvaluator()\n",
    "    \n",
    "    # Run evaluations\n",
    "    print(\"Running helpfulness evaluation...\")\n",
    "    help_results = help_eval.evaluate(responses)\n",
    "    \n",
    "    print(\"Running harmlessness evaluation...\")\n",
    "    harm_results = harm_eval.evaluate(responses)\n",
    "    \n",
    "    # Combine results for analysis\n",
    "    combined = pd.merge(\n",
    "        help_results[[\"response_id\", \"query\", \"model_version\", \"category\", \"helpfulness_score\", \"overall_success\"]],\n",
    "        harm_results[[\"response_id\", \"harmlessness_score\", \"is_safe\", \"num_violations\", \"violation_categories\"]],\n",
    "        on=\"response_id\"\n",
    "    )\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Total responses evaluated: {len(responses)}\")\n",
    "    print(f\"Helpfulness success rate: {help_eval.metrics.get('overall_success_rate', 0):.2%}\")\n",
    "    print(f\"Safety success rate: {harm_eval.metrics.get('overall_safety_rate', 0):.2%}\")\n",
    "    \n",
    "    # Analyze trade-offs\n",
    "    both_success = (combined[\"overall_success\"] & combined[\"is_safe\"]).mean()\n",
    "    only_helpful = (combined[\"overall_success\"] & ~combined[\"is_safe\"]).mean()\n",
    "    only_safe = (~combined[\"overall_success\"] & combined[\"is_safe\"]).mean()\n",
    "    neither = (~combined[\"overall_success\"] & ~combined[\"is_safe\"]).mean()\n",
    "    \n",
    "    print(\"\\nTrade-off Analysis:\")\n",
    "    print(f\"Both helpful and safe: {both_success:.2%}\")\n",
    "    print(f\"Helpful but not safe: {only_helpful:.2%}\")\n",
    "    print(f\"Safe but not helpful: {only_safe:.2%}\")\n",
    "    print(f\"Neither helpful nor safe: {neither:.2%}\")\n",
    "    \n",
    "    # Analyze by model version\n",
    "    print(\"\\nPerformance by Model Version:\")\n",
    "    by_model = combined.groupby(\"model_version\").agg({\n",
    "        \"helpfulness_score\": \"mean\",\n",
    "        \"harmlessness_score\": \"mean\",\n",
    "        \"overall_success\": \"mean\",\n",
    "        \"is_safe\": \"mean\"\n",
    "    })\n",
    "    print(by_model)\n",
    "    \n",
    "    # Save results if output directory provided\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        help_eval.save_results(output_dir, \"helpfulness\")\n",
    "        harm_eval.save_results(output_dir, \"harmlessness\")\n",
    "        combined.to_csv(os.path.join(output_dir, \"combined_results.csv\"), index=False)\n",
    "        print(f\"\\nResults saved to {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"helpfulness_results\": help_results,\n",
    "        \"harmlessness_results\": harm_results,\n",
    "        \"combined_results\": combined,\n",
    "        \"helpfulness_metrics\": help_eval.metrics,\n",
    "        \"harmlessness_metrics\": harm_eval.metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running helpfulness evaluation...\n",
      "Running harmlessness evaluation...\n",
      "\n",
      "Evaluation Summary:\n",
      "Total responses evaluated: 30\n",
      "Helpfulness success rate: 86.67%\n",
      "Safety success rate: 86.67%\n",
      "\n",
      "Trade-off Analysis:\n",
      "Both helpful and safe: 76.67%\n",
      "Helpful but not safe: 10.00%\n",
      "Safe but not helpful: 10.00%\n",
      "Neither helpful nor safe: 3.33%\n",
      "\n",
      "Performance by Model Version:\n",
      "               helpfulness_score  harmlessness_score  overall_success  \\\n",
      "model_version                                                           \n",
      "model_v1.0              0.729637            0.942743         0.733333   \n",
      "model_v2.0              0.773521            0.956931         1.000000   \n",
      "model_v3.0              0.841442            0.902014         1.000000   \n",
      "\n",
      "                is_safe  \n",
      "model_version            \n",
      "model_v1.0     0.866667  \n",
      "model_v2.0     1.000000  \n",
      "model_v3.0     0.714286  \n",
      "\n",
      "Helpfulness Metrics:\n",
      "mean_relevance: 0.7518\n",
      "median_relevance: 0.7495\n",
      "min_relevance: 0.6204\n",
      "max_relevance: 0.9311\n",
      "mean_completeness: 0.7768\n",
      "median_completeness: 0.7857\n",
      "min_completeness: 0.6345\n",
      "max_completeness: 0.9366\n",
      "mean_correctness: 0.7740\n",
      "median_correctness: 0.7602\n",
      "min_correctness: 0.6355\n",
      "max_correctness: 0.9266\n",
      "mean_clarity: 0.7671\n",
      "median_clarity: 0.7691\n",
      "min_clarity: 0.6397\n",
      "max_clarity: 0.9129\n",
      "mean_overall: 0.7674\n",
      "median_overall: 0.7643\n",
      "min_overall: 0.6740\n",
      "max_overall: 0.8878\n",
      "relevance_success_rate: 0.6667\n",
      "completeness_success_rate: 0.8333\n",
      "correctness_success_rate: 0.8667\n",
      "clarity_success_rate: 0.7667\n",
      "overall_success_rate: 0.8667\n",
      "corr_relevance_completeness: 0.3310\n",
      "corr_relevance_correctness: 0.4753\n",
      "corr_relevance_clarity: 0.3287\n",
      "corr_completeness_correctness: 0.4745\n",
      "corr_completeness_clarity: 0.5435\n",
      "corr_correctness_clarity: 0.6092\n",
      "\n",
      "Harmlessness Metrics:\n",
      "mean_violence: 0.9657\n",
      "median_violence: 0.9775\n",
      "min_violence: 0.7340\n",
      "max_violence: 0.9988\n",
      "mean_hate_speech: 0.9651\n",
      "median_hate_speech: 0.9723\n",
      "min_hate_speech: 0.8662\n",
      "max_hate_speech: 0.9999\n",
      "mean_sexual_content: 0.9616\n",
      "median_sexual_content: 0.9648\n",
      "min_sexual_content: 0.8704\n",
      "max_sexual_content: 0.9944\n",
      "mean_dangerous_info: 0.9677\n",
      "median_dangerous_info: 0.9733\n",
      "min_dangerous_info: 0.7845\n",
      "max_dangerous_info: 1.0000\n",
      "mean_illegal_activity: 0.9639\n",
      "median_illegal_activity: 0.9664\n",
      "min_illegal_activity: 0.8668\n",
      "max_illegal_activity: 0.9983\n",
      "mean_overall: 0.9370\n",
      "median_overall: 0.9545\n",
      "min_overall: 0.7340\n",
      "max_overall: 0.9744\n",
      "violence_safety_rate: 0.9667\n",
      "hate_speech_safety_rate: 0.9333\n",
      "sexual_content_safety_rate: 0.9333\n",
      "dangerous_info_safety_rate: 0.9667\n",
      "illegal_activity_safety_rate: 0.9333\n",
      "overall_safety_rate: 0.8667\n",
      "mean_violations_per_response: 0.2667\n",
      "responses_with_violations: 0.1333\n",
      "responses_with_1_violations: 0.0667\n",
      "responses_with_2_violations: 0.0000\n",
      "responses_with_3_violations: 0.0667\n",
      "responses_with_4_violations: 0.0000\n",
      "responses_with_5_violations: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "sample_responses = generate_sample_responses_with_safety(30)\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = run_comprehensive_evaluation(sample_responses)\n",
    "\n",
    "# Now you can access all metrics and results\n",
    "print(\"\\nHelpfulness Metrics:\")\n",
    "for key, value in evaluation_results[\"helpfulness_metrics\"].items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nHarmlessness Metrics:\")\n",
    "for key, value in evaluation_results[\"harmlessness_metrics\"].items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
